# Checkpoint ç»“æ„è¯´æ˜ä¸éªŒè¯æŒ‡å—

## ğŸ“ Checkpoint æ–‡ä»¶ç»“æ„

ä½ çš„ checkpoint ä½äºï¼š`output/v30-20251117-203755/checkpoint-96/`

### æ–‡ä»¶åˆ—è¡¨åŠè¯´æ˜

```
checkpoint-96/
â”œâ”€â”€ adapter_model.safetensors    # LoRAæƒé‡æ–‡ä»¶ï¼ˆ56MBï¼‰- æ ¸å¿ƒæ–‡ä»¶
â”œâ”€â”€ adapter_config.json          # LoRAé…ç½®ï¼ˆrank, alpha, dropoutç­‰ï¼‰
â”œâ”€â”€ trainer_state.json           # è®­ç»ƒçŠ¶æ€ï¼ˆloss, step, epochç­‰ï¼‰
â”œâ”€â”€ training_args.bin            # è®­ç»ƒå‚æ•°ï¼ˆäºŒè¿›åˆ¶ï¼‰
â”œâ”€â”€ sft_args.json               # SFTè®­ç»ƒå‚æ•°ï¼ˆJSONæ ¼å¼ï¼‰
â”œâ”€â”€ configuration.json           # æ¨¡å‹é…ç½®
â”œâ”€â”€ generation_config.json       # ç”Ÿæˆé…ç½®
â”œâ”€â”€ additional_config.json       # é¢å¤–é…ç½®
â””â”€â”€ README.md                    # æ¨¡å‹å¡ç‰‡ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
```

---

## ğŸ“Š å…³é”®æ–‡ä»¶è¯¦è§£

### 1. `adapter_model.safetensors` - LoRAæƒé‡

**ä½œç”¨**ï¼šåŒ…å«æ‰€æœ‰å¯è®­ç»ƒçš„LoRAæƒé‡å‚æ•°

**å¤§å°**ï¼š56MBï¼ˆç›¸å¯¹è¾ƒå°ï¼Œå› ä¸ºåªä¿å­˜LoRAå‚æ•°ï¼Œä¸æ˜¯å®Œæ•´æ¨¡å‹ï¼‰

**å†…å®¹**ï¼š
- LoRAçš„AçŸ©é˜µå’ŒBçŸ©é˜µæƒé‡
- åªåŒ…å«è®­ç»ƒæ—¶æ›´æ–°çš„å‚æ•°ï¼ˆrank=8, alpha=32ï¼‰

**é‡è¦æ€§**ï¼šâ­â­â­â­â­ **è¿™æ˜¯æœ€æ ¸å¿ƒçš„æ–‡ä»¶**

---

### 2. `adapter_config.json` - LoRAé…ç½®

**å†…å®¹**ï¼š
```json
{
  "base_model_name_or_path": "/home/hmpiao/hmpiao/Qwen2-VL-2B-Instruct",
  "peft_type": "LORA",
  "r": 8,                    // LoRA rank
  "lora_alpha": 32,          // LoRA alpha
  "lora_dropout": 0.05,      // LoRA dropout
  "target_modules": "^(model)(?!.*(lm_head|output|emb|wte|shared)).*",
  "task_type": "CAUSAL_LM"
}
```

**ä½œç”¨**ï¼šå‘Šè¯‰åŠ è½½å™¨å¦‚ä½•å°†LoRAæƒé‡åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹

---

### 3. `trainer_state.json` - è®­ç»ƒçŠ¶æ€

**å…³é”®ä¿¡æ¯**ï¼š
```json
{
  "global_step": 96,              // æ€»è®­ç»ƒæ­¥æ•°
  "epoch": 1.0,                   // è®­ç»ƒè½®æ•°
  "max_steps": 96,                // æœ€å¤§æ­¥æ•°
  "log_history": [
    {
      "step": 1,
      "loss": 2.12130213,         // è®­ç»ƒæŸå¤±
      "acc": 0.49849987,           // å‡†ç¡®ç‡ï¼ˆ49.85%ï¼‰
      "learning_rate": 1.67e-05,   // å­¦ä¹ ç‡
      "grad_norm": 2.42,           // æ¢¯åº¦èŒƒæ•°
      "memory(GiB)": 14.81,        // æ˜¾å­˜å ç”¨
      "train_speed(iter/s)": 0.056 // è®­ç»ƒé€Ÿåº¦
    }
  ]
}
```

**åˆ†æ**ï¼š
- âœ… **Loss**: 2.12ï¼ˆåˆå§‹æŸå¤±ï¼Œéœ€è¦æ›´å¤šæ­¥æ•°æ‰èƒ½çœ‹åˆ°ä¸‹é™ï¼‰
- âš ï¸ **Acc**: 49.85%ï¼ˆæ¥è¿‘éšæœºçŒœæµ‹ï¼Œè¯´æ˜è®­ç»ƒåˆšå¼€å§‹ï¼‰
- âœ… **æ˜¾å­˜**: 14.81 GiBï¼ˆä¼˜åŒ–æˆåŠŸï¼Œæ²¡æœ‰OOMï¼‰
- âœ… **è®­ç»ƒé€Ÿåº¦**: 0.056 iter/sï¼ˆçº¦18ç§’/æ­¥ï¼‰

---

### 4. `sft_args.json` - è®­ç»ƒå‚æ•°

**å…³é”®å‚æ•°**ï¼š
```json
{
  "model_type": "qwen2-vl-2b-instruct",
  "sft_type": "lora",
  "lora_rank": 8,
  "lora_alpha": 32,
  "max_length": 1024,
  "learning_rate": 5e-5,
  "batch_size": 1,
  "gradient_accumulation_steps": 16,
  "max_steps": 96,
  "dataset": ["/home/hmpiao/.../episode-wise-conversations.jsonl"]
}
```

---

## ğŸ” å¦‚ä½•éªŒè¯è®­ç»ƒæ•ˆæœ

### æ–¹æ³• 1: ä½¿ç”¨ Swift Infer è¿›è¡Œæ¨ç†æµ‹è¯•ï¼ˆæ¨èï¼‰

#### æ­¥éª¤ 1: å‡†å¤‡æµ‹è¯•æ•°æ®

åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆJSONLæ ¼å¼ï¼‰ï¼Œä¾‹å¦‚ `test_data.jsonl`ï¼š

```jsonl
{"images": ["path/to/image1.png"], "query": "What is in this image?", "response": "..."}
{"images": ["path/to/image2.png"], "query": "Click the button", "response": "..."}
```

#### æ­¥éª¤ 2: è¿è¡Œæ¨ç†

```bash
# å•GPUæ¨ç†
CUDA_VISIBLE_DEVICES=0 \
MAX_PIXELS=200000 \
swift infer \
  --ckpt_dir output/v30-20251117-203755/checkpoint-96 \
  --val_dataset test_data.jsonl \
  --model_type qwen2-vl-2b-instruct \
  --model_id_or_path /home/hmpiao/hmpiao/Qwen2-VL-2B-Instruct \
  --sft_type lora \
  --max_length 1024
```

**è¾“å‡º**ï¼šä¼šåœ¨ `checkpoint-96/infer_result/` ç›®å½•ä¸‹ç”Ÿæˆæ¨ç†ç»“æœ

---

### æ–¹æ³• 2: ä½¿ç”¨ Python ä»£ç åŠ è½½æ¨¡å‹

```python
from swift.llm import get_model_tokenizer
from swift.tuners import Swift
from peft import PeftModel
from transformers import AutoProcessor
import torch

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
model, tokenizer = get_model_tokenizer(
    model_type='qwen2-vl-2b-instruct',
    model_id_or_path='/home/hmpiao/hmpiao/Qwen2-VL-2B-Instruct',
    torch_dtype=torch.bfloat16,
    device_map='auto'
)

# 2. åŠ è½½LoRAæƒé‡
model = PeftModel.from_pretrained(
    model,
    'output/v30-20251117-203755/checkpoint-96'
)

# 3. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

# 4. æµ‹è¯•æ¨ç†
processor = tokenizer.processor
image = Image.open('test_image.png')
query = "What is in this image?"

# å‡†å¤‡è¾“å…¥
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": query}
        ]
    }
]

# ç”Ÿæˆå›å¤
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = processor.process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt"
).to(model.device)

with torch.no_grad():
    generated_ids = model.generate(**inputs, max_new_tokens=512)
    generated_ids_trimmed = [
        out_ids[len(in_ids):] 
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    response = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]

print(f"Response: {response}")
```

---

### æ–¹æ³• 3: ä½¿ç”¨é¡¹ç›®æä¾›çš„è¯„ä¼°è„šæœ¬

æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„ï¼Œå¯ä»¥ä½¿ç”¨ `evaluation/` ç›®å½•ä¸‹çš„è„šæœ¬ï¼š

```bash
# 1. è¿è¡Œæ¨ç†
CUDA_VISIBLE_DEVICES=0 \
MAX_PIXELS=200000 \
swift infer \
  --ckpt_dir output/v30-20251117-203755/checkpoint-96 \
  --val_dataset /path/to/val_dataset.jsonl \
  --model_type qwen2-vl-2b-instruct \
  --model_id_or_path /home/hmpiao/hmpiao/Qwen2-VL-2B-Instruct \
  --sft_type lora

# 2. è®¡ç®—å‡†ç¡®ç‡
python evaluation/test_swift.py \
  --data_path output/v30-20251117-203755/checkpoint-96/infer_result/*.jsonl
```

---

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡è¯´æ˜

### 1. **Lossï¼ˆæŸå¤±ï¼‰**

**æœŸæœ›è¶‹åŠ¿**ï¼š
- âœ… è®­ç»ƒè¿‡ç¨‹ä¸­åº”è¯¥**é€æ¸ä¸‹é™**
- âš ï¸ å½“å‰å€¼ï¼š2.12ï¼ˆè¾ƒé«˜ï¼Œå› ä¸ºåªè®­ç»ƒäº†96æ­¥ï¼‰

**åˆ¤æ–­æ ‡å‡†**ï¼š
- Loss < 1.0ï¼šè®­ç»ƒè‰¯å¥½
- Loss < 0.5ï¼šè®­ç»ƒå¾ˆå¥½
- Loss < 0.1ï¼šå¯èƒ½è¿‡æ‹Ÿåˆ

---

### 2. **Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰**

**å½“å‰å€¼**ï¼š49.85%

**åˆ¤æ–­æ ‡å‡†**ï¼š
- å¯¹äºAndroid Controlä»»åŠ¡ï¼š
  - **éšæœºçŒœæµ‹**: ~25-33%ï¼ˆå–å†³äºåŠ¨ä½œç©ºé—´ï¼‰
  - **åŸºçº¿æ¨¡å‹**: ~40-50%
  - **è‰¯å¥½æ¨¡å‹**: >60%
  - **ä¼˜ç§€æ¨¡å‹**: >75%

**âš ï¸ æ³¨æ„**ï¼šå½“å‰å‡†ç¡®ç‡æ¥è¿‘éšæœºï¼Œå› ä¸ºï¼š
- åªè®­ç»ƒäº†96æ­¥ï¼ˆ1ä¸ªepochï¼‰
- éœ€è¦æ›´å¤šè®­ç»ƒæ­¥æ•°æ‰èƒ½çœ‹åˆ°æ˜æ˜¾æå‡

---

### 3. **è®­ç»ƒç¨³å®šæ€§**

æ£€æŸ¥ `trainer_state.json` ä¸­çš„ï¼š
- **grad_norm**: 2.42ï¼ˆæ­£å¸¸ï¼Œè¯´æ˜æ¢¯åº¦æ²¡æœ‰çˆ†ç‚¸ï¼‰
- **learning_rate**: 1.67e-05ï¼ˆæ­£å¸¸è¡°å‡ï¼‰
- **memory**: 14.81 GiBï¼ˆç¨³å®šï¼Œæ²¡æœ‰OOMï¼‰

---

## ğŸ¯ éªŒè¯ Checklist

### âœ… åŸºç¡€æ£€æŸ¥

- [x] Checkpointæ–‡ä»¶å®Œæ•´ï¼ˆæ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨ï¼‰
- [x] LoRAæƒé‡æ–‡ä»¶å¤§å°åˆç†ï¼ˆ56MBï¼Œç¬¦åˆrank=8ï¼‰
- [x] è®­ç»ƒå®Œæˆï¼ˆglobal_step = max_steps = 96ï¼‰
- [x] æ²¡æœ‰OOMé”™è¯¯
- [x] æ¢¯åº¦æ­£å¸¸ï¼ˆgrad_norm < 10ï¼‰

### âš ï¸ éœ€è¦æ”¹è¿›

- [ ] Lossä»ç„¶è¾ƒé«˜ï¼ˆ2.12ï¼‰ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒ
- [ ] Accuracyè¾ƒä½ï¼ˆ49.85%ï¼‰ï¼Œæ¥è¿‘éšæœºçŒœæµ‹
- [ ] åªè®­ç»ƒäº†96æ­¥ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ­¥æ•°

---

## ğŸ”§ å¦‚ä½•ç»§ç»­è®­ç»ƒ

å¦‚æœéœ€è¦ç»§ç»­è®­ç»ƒï¼Œå¯ä»¥ä»checkpointæ¢å¤ï¼š

```bash
CUDA_VISIBLE_DEVICES=0,1 \
MAX_PIXELS=200000 \
swift sft \
  --resume_from_checkpoint output/v30-20251117-203755/checkpoint-96 \
  --model_type qwen2-vl-2b-instruct \
  --model_id_or_path /home/hmpiao/hmpiao/Qwen2-VL-2B-Instruct \
  --dataset /home/hmpiao/.../episode-wise-conversations.jsonl \
  --max_steps 500 \  # å¢åŠ è®­ç»ƒæ­¥æ•°
  --save_steps 100 \
  ...å…¶ä»–å‚æ•°...
```

---

## ğŸ“ å¿«é€Ÿæµ‹è¯•è„šæœ¬

åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬ `test_checkpoint.sh`ï¼š

```bash
#!/bin/bash
# æµ‹è¯•checkpointæ•ˆæœ

CKPT_DIR="output/v30-20251117-203755/checkpoint-96"
MODEL_TYPE="qwen2-vl-2b-instruct"
MODEL_PATH="/home/hmpiao/hmpiao/Qwen2-VL-2B-Instruct"
TEST_DATA="test_data.jsonl"  # ä½ çš„æµ‹è¯•æ•°æ®

echo "=== 1. è¿è¡Œæ¨ç† ==="
CUDA_VISIBLE_DEVICES=0 \
MAX_PIXELS=200000 \
swift infer \
  --ckpt_dir "$CKPT_DIR" \
  --val_dataset "$TEST_DATA" \
  --model_type "$MODEL_TYPE" \
  --model_id_or_path "$MODEL_PATH" \
  --sft_type lora \
  --max_length 1024

echo "=== 2. è®¡ç®—å‡†ç¡®ç‡ ==="
python evaluation/test_swift.py \
  --data_path "$CKPT_DIR/infer_result/*.jsonl"

echo "=== 3. æŸ¥çœ‹è®­ç»ƒæ—¥å¿— ==="
cat "$CKPT_DIR/trainer_state.json" | python -m json.tool
```

---

## ğŸ“ æ€»ç»“

### Checkpointç»“æ„
- âœ… **LoRAæƒé‡** (`adapter_model.safetensors`) - æ ¸å¿ƒæ–‡ä»¶
- âœ… **é…ç½®** (`adapter_config.json`) - åŠ è½½é…ç½®
- âœ… **è®­ç»ƒçŠ¶æ€** (`trainer_state.json`) - è®­ç»ƒä¿¡æ¯

### å½“å‰çŠ¶æ€
- âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼ˆ96æ­¥ï¼‰
- âœ… æ²¡æœ‰OOMé—®é¢˜
- âš ï¸ Losså’ŒAccuracyéœ€è¦æ›´å¤šè®­ç»ƒæ‰èƒ½æ”¹å–„

### éªŒè¯æ–¹æ³•
1. **Swift Infer** - æ¨èï¼Œæœ€ç®€å•
2. **Pythonä»£ç ** - çµæ´»ï¼Œå¯è‡ªå®šä¹‰
3. **è¯„ä¼°è„šæœ¬** - é¡¹ç›®æä¾›ï¼Œæ ‡å‡†åŒ–

### ä¸‹ä¸€æ­¥
1. è¿è¡Œæ¨ç†æµ‹è¯•ï¼ŒæŸ¥çœ‹å®é™…æ•ˆæœ
2. å¦‚æœæ•ˆæœä¸ä½³ï¼Œç»§ç»­è®­ç»ƒæ›´å¤šæ­¥æ•°
3. è°ƒæ•´è¶…å‚æ•°ï¼ˆlearning_rate, batch_sizeç­‰ï¼‰

